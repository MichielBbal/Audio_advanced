{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97a13bc",
   "metadata": {},
   "source": [
    "# Audio classification on the Urban Sounds dataset with CLIP and AST\n",
    "\n",
    "In this notebook you can do audio classification on the Urban Sounds dataset. \n",
    "This dataset contains of nine classes of audio events, such as motor cycle, screaming people or gunshots.\n",
    "\n",
    "The dataset is hosted on the Huggingface Hub at: https://huggingface.co/datasets/UrbanSounds/urban_sounds_smal\n",
    "\n",
    "Two AI models are used in this notebook:\n",
    "- CLAP \n",
    "- Audio Spectrum Transformer (AST)\n",
    "\n",
    "### Contents\n",
    "0. Install packages\n",
    "1. CLAP on the Urban Sounds Amsterdam dataset\n",
    "2. AST on the Urban Sounds Amsterdam dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886e42d",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "### Using datasets\n",
    "To classify sounds, I've uploaded a sub-sample of the Amsterdam Sounds Database at Huggingface. \n",
    "\n",
    "In this notebook we will use Huggingface's ```dataset``` library to load this dataset. \n",
    "\n",
    "**links & sources**:\n",
    "- https://pypi.org/project/datasets/\n",
    "- https://huggingface.co/docs/datasets/audio_process\n",
    "- https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Audio\n",
    "- https://huggingface.co/learn/audio-course/chapter2/audio_classification_pipeline?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d66e31",
   "metadata": {},
   "source": [
    "# 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24640679",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\\[audio\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check numpy version is 1.24\n",
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d85485",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1deef",
   "metadata": {},
   "source": [
    "## 1. CLAP on the Urban Sounds Amsterdam dataset\n",
    "\n",
    "Source: https://huggingface.co/laion/larger_clap_general\n",
    "\n",
    "Paper: https://arxiv.org/abs/2211.06687\n",
    "\n",
    "CLAP (Contrastive Language-Audio Pretraining) is a neural network trained on a variety of (audio, text) pairs. It can be instructed in to predict the most relevant text snippet, given an audio, without directly optimizing for the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4888e0",
   "metadata": {},
   "source": [
    "### Inspection of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d31b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"UrbanSounds/urban_sounds_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect one sample from \n",
    "example = dataset['train']['audio'][0]\n",
    "label = dataset['train']['label'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612c868",
   "metadata": {},
   "source": [
    "You may notice that the audio column contains several features. Hereâ€™s what they are:\n",
    "\n",
    "- path: the path to the downloaded (and converted) audio file\n",
    "- array: The decoded audio data, represented as a 1-dimensional NumPy array.\n",
    "- sampling_rate. The sampling rate of the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc23643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the label data\n",
    "print(dataset['train']['label'])\n",
    "print(len(dataset['train']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb34b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting the audio array\n",
    "array = dataset[\"train\"][\"audio\"][0][\"array\"]\n",
    "sampling_rate = example[\"sampling_rate\"]\n",
    "print(array.shape)\n",
    "print(array)\n",
    "print(type(array))\n",
    "print(sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf09080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "plt.figure().set_figwidth(10)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the display script\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(example[\"array\"], rate=example['sampling_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b333ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check length of dataset\n",
    "print(len(dataset['train']['audio']))\n",
    "print(type(dataset['train']['audio']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to load a random number out of the dataset\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import IPython\n",
    "import random\n",
    "\n",
    "#creating a random number\n",
    "random_number = random.randint(0, len(dataset['train']['audio']))\n",
    "\n",
    "example=dataset['train']['audio'][random_number]\n",
    "audio = dataset[\"train\"][\"audio\"][random_number][\"array\"]\n",
    "\n",
    "audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/larger_clap_music_and_speech\")\n",
    "output = audio_classifier(audio, candidate_labels=[\"Motorcycle\", \"Moped\", 'Claxon','Alarm', 'Silence','Loud people','Talking','Gunshot', 'Slamming door','Music'])\n",
    "print(output[0],'\\n',output[1])\n",
    "print(random_number)\n",
    "IPython.display.Audio(example[\"array\"], rate=example['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c17f74",
   "metadata": {},
   "source": [
    "## CLAP on the urban_sounds_small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae117fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"UrbanSounds/urban_sounds_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4171af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary the converts the class folders to real names\n",
    "label_dict ={0:'Gunshot', 1:'Moped alarm', 2:'Moped', 3:'Claxon', 4:'Slamming door', 5:'Screaming', 6:'Motorcycle', 7:'Talking', 8:'Music'}\n",
    "print('The given labels are: ')\n",
    "for i in range(0,9):\n",
    "    print(label_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b09319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the item from the dataset < 223\n",
    "i = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4328d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#larger_clap_general\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import IPython\n",
    "\n",
    "#dataset = load_dataset(\"MichielBontenbal/UrbanSounds\")\n",
    "\n",
    "example=dataset['train']['audio'][i]\n",
    "audio = dataset[\"train\"][\"audio\"][i]['array']\n",
    "\n",
    "audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/larger_clap_general\")\n",
    "output = audio_classifier(audio, candidate_labels=[\"Gunshot\", \"Moped\", 'Moped alarm','Claxon','Screaming', 'Motorcycle','Talking', 'Slamming door','Music', 'Silence'])\n",
    "\n",
    "predicted_label = output[0]['label']\n",
    "print(f'Predicted label: {predicted_label}')\n",
    "\n",
    "label_name =label_dict[dataset['train']['label'][i]]\n",
    "print(f'The given label: {label_name}')\n",
    "\n",
    "if label_name == output[0]['label']:\n",
    "    print(\"This is correct\")\n",
    "else:\n",
    "    print('This is false')\n",
    "print(f'Probability: {round(output[0][\"score\"],3)}')\n",
    "\n",
    "IPython.display.Audio(example['array'], rate=example['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e025a68",
   "metadata": {},
   "source": [
    "## Code as a function (same code as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code above as a function\n",
    "from transformers import pipeline\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display #use display to create a audio player in a function\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"UrbanSounds/urban_sounds_small\")\n",
    "\n",
    "def process_audio(i, dataset):\n",
    "    example = dataset['train']['audio'][i]\n",
    "    audio = dataset[\"train\"][\"audio\"][i]['array']\n",
    "    return example, audio\n",
    "\n",
    "def classify_audio(audio, model=\"laion/larger_clap_general\"):\n",
    "    audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=model)\n",
    "    output_var = audio_classifier(audio, candidate_labels=[\"Gunshot\", \"Moped\", 'Moped alarm','Claxon','Screaming', 'Motorcycle','Talking', 'Slamming door','Music', 'Silence'])\n",
    "    print(output_var[0])\n",
    "    return output_var\n",
    "\n",
    "def display_results(output, i, dataset, label_dict):\n",
    "    predicted_label = output[0]['label']\n",
    "    print(f'Predicted label: {predicted_label}')\n",
    "    \n",
    "    label_name = label_dict[dataset['train']['label'][i]]\n",
    "    print(f'The given label: {label_name}')\n",
    "    \n",
    "    if label_name == output[0]['label']:\n",
    "        print(\"This is correct\")\n",
    "    else:\n",
    "        print('This is wrong')\n",
    "\n",
    "    probability = output[0]['score']\n",
    "    print(f'Probability: {round(probability,3)}')\n",
    "    #IPython.display.display.Audio(dataset['train']['audio'][i]['array'], rate=dataset['train']['audio'][i]['sampling_rate'])\n",
    "\n",
    "    display(Audio(dataset['train']['audio'][i]['array'], rate=dataset['train']['audio'][i]['sampling_rate']))\n",
    "    \n",
    "    return predicted_label, label_name, probability\n",
    "\n",
    "def main(i):\n",
    "    #dataset = load_audio_dataset()\n",
    "    \n",
    "    # Replace 'i' with the appropriate index\n",
    "    #i = 0\n",
    "    \n",
    "    example, audio = process_audio(i, dataset)\n",
    "    output = classify_audio(audio)\n",
    "    display_results(output, i, dataset, label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3425e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52899cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "def cpu_usage_monitor(interval=1):\n",
    "    \"\"\"\n",
    "    Monitors CPU usage every 'interval' seconds.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            cpu_usage = psutil.cpu_percent(interval=interval)\n",
    "            print(f\"CPU Usage: {cpu_usage}%\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"CPU monitoring stopped.\")\n",
    "\n",
    "def another_task():\n",
    "    \"\"\"\n",
    "    A placeholder function for another task.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            # Replace this with your code for the other task\n",
    "            example, audio = process_audio(i, dataset)\n",
    "            output = classify_audio(audio)\n",
    "            display_results(output, i, dataset, label_dict)\n",
    "            time.sleep(2)  # Example delay\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Another task stopped.\")\n",
    "\n",
    "# Create threads\n",
    "thread_cpu_monitor = threading.Thread(target=cpu_usage_monitor, args=(1,))\n",
    "thread_another_task = threading.Thread(target=another_task)\n",
    "\n",
    "# Start threads\n",
    "thread_cpu_monitor.start()\n",
    "thread_another_task.start()\n",
    "\n",
    "# Wait for the threads to finish (optional)\n",
    "thread_cpu_monitor.join()\n",
    "thread_another_task.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa586a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)\n",
    "\n",
    "print(50 * '-')\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3afb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(output):\n",
    "    predicted_label = output[0]['label']\n",
    "    print(f'Predicted label: {predicted_label}')\n",
    "    return predicted_label\n",
    "\n",
    "predict_label(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_given_label(dataset, i):\n",
    "    label_name = label_dict[dataset['train']['label'][i]]\n",
    "    print(f'The given label: {label_name}')\n",
    "    return label_name\n",
    "\n",
    "get_given_label(dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35dd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(output):\n",
    "    probability = output[0]['score']\n",
    "    print(probability)\n",
    "    return probability\n",
    "\n",
    "get_prob(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = predict_label(output)\n",
    "given_label = get_given_label(dataset, i)\n",
    "probability = get_prob(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeaca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_label)\n",
    "print(given_label)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code above as a function\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display #use display to create a audio player in a function\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"UrbanSounds/urban_sounds_small\")\n",
    "\n",
    "def process_audio(i, dataset):\n",
    "    example = dataset['train']['audio'][i]\n",
    "    audio = dataset[\"train\"][\"audio\"][i]['array']\n",
    "    return example, audio\n",
    "\n",
    "def classify_audio(audio, model=\"laion/larger_clap_general\"):\n",
    "    audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=model)\n",
    "    output = audio_classifier(audio, candidate_labels=[\"Gunshot\", \"Moped\", 'Moped alarm','Claxon','Screaming', 'Motorcycle','Talking', 'Slamming door','Music', 'Silence'])\n",
    "    return output\n",
    "\n",
    "def predict_label(output):\n",
    "    predicted_label = output[0]['label']\n",
    "    print(f'Predicted label: {predicted_label}')\n",
    "    return predicted_label\n",
    "    \n",
    "def get_given_label(dataset, i):\n",
    "    label_name = label_dict[dataset['train']['label'][i]]\n",
    "    print(f'The given label: {label_name}')\n",
    "    return label_name\n",
    "    \n",
    "def get_prob(output):\n",
    "    probability = output[0]['score']\n",
    "    print(probability)\n",
    "    return probability\n",
    "\n",
    "def diplay_audio(dataset, i):\n",
    "    display(Audio(dataset['train']['audio'][i]['array'], rate=dataset['train']['audio'][i]['sampling_rate']))\n",
    "\n",
    "def main2(i):\n",
    "    #dataset = load_audio_dataset()\n",
    "    # Replace 'i' with the appropriate index\n",
    "    \n",
    "    example, audio = process_audio(i, dataset)\n",
    "    output = classify_audio(audio)\n",
    "    predicted_label = predict_label(output)\n",
    "    given_label = get_given_label(dataset, i)\n",
    "    probability = get_prob(output)\n",
    "    diplay_audio(dataset, i)\n",
    "    return output, predicted_label\n",
    "    print(output)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624bb87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main2(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list=[]\n",
    "for i in range(0,223):\n",
    "    example, audio = process_audio(i, dataset)\n",
    "    output = classify_audio(audio)\n",
    "    predicted_label = predict_label(output)\n",
    "    given_label = get_given_label(dataset, i)\n",
    "    probability = get_prob(output)\n",
    "    result_list.append([predicted_label, given_label, probability])\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Your Python list\n",
    "#my_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Specify the file path where you want to save the JSON file\n",
    "file_path = 'result_list.json'\n",
    "\n",
    "# Open the file in write mode and use json.dump to write the list to the file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(result_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37923036",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat result_list.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example)\n",
    "print(audio)\n",
    "print(output[0])\n",
    "print(output[0]['label'])\n",
    "print(predicted_label)\n",
    "print(given_label)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a4b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04b3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21df9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 222, 20):\n",
    "    main(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79bff3",
   "metadata": {},
   "source": [
    "# 2. Audio Spectrum Transfomer with Urban Sounds Amsterdam Dataset\n",
    "\n",
    "Model: https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593\n",
    "Paper: https://arxiv.org/abs/2104.01778\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"UrbanSounds/urban_sounds_small\")\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset['train'][\"audio\"][1][\"array\"], sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "predicted_label = model.config.id2label[predicted_class_ids]\n",
    "print(predicted_label)\n",
    "\n",
    "# compute loss - target_label is e.g. \"down\"\n",
    "target_label = model.config.id2label[0]\n",
    "inputs[\"labels\"] = torch.tensor([model.config.label2id[target_label]])\n",
    "loss = model(**inputs).loss\n",
    "print(round(loss.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d771a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "example=dataset['train'][\"audio\"][1]\n",
    "IPython.display.Audio(example[\"array\"], rate=example['sampling_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1a690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d70ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
